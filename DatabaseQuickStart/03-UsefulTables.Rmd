# Useful Tables

```{r dbConnect, echo=F, warning=F}
library(DBI)
library(RMySQL)
library(data.table)

############# SQL CONNECTION ###############
getSqlConnection <- function(){
  con <-
    dbConnect(
      RMySQL::MySQL(),
      username = Sys.getenv('user'),
      password = Sys.getenv('pswd'),
      host = Sys.getenv('ipAddress'),
      dbname = 'Octoparse'
    )
  return(con)
}

database_connection <- getSqlConnection()
tables_list <- dbListTables(database_connection)

query <- 'SELECT table_schema "DB name", sum( data_length + index_length ) / 1024 / 1024 "Size in MB", curdate() as Today 
FROM information_schema.TABLES GROUP BY table_schema 
ORDER BY `Size in MB` desc limit 2'

tables_summary <- dbFetch(dbSendQuery(database_connection, query))
```

CONNECTED TO DB. CREATE WEEKLY SUMMARIES OF DATA FLOWING IN BY DATE, AND SHOW EXAMPLE OF LATEST DATA COLLECTED!!

## Browse Data

Let's start by clicking on the `Browse Data` tab in the top right of the Metabase environment:

![](images/MetabaseBrowseData.png)


Here, you will see two different options:

![](images/DBSelection.png)

- `Octoparse` is the schema that is associated with data I have collected by using the *Octoparse* web scraping software.

- Conversely, `ScrapeStorm` is the schema associated with data collected using the *ScrapeStorm* web scraping software.

- Web scraping has its challenges in terms of stability, so I built some additional resilience by using two different tools that work independently of each other and do similar things (and in some cases collect the same data). Although not a perfect solution, having both up and running means we can usually fill the gaps that might arise in each tool respectively.

    + The web scrapers that are run through Octoparse run on their servers in the cloud, which are very stable but have had some issues here and there in the past.
    
    + The web scrapers that are run through ScrapeStorm run on a local machine on my end. I have a computer in the cloud that runs 24/7 but even after upgrading the hardware on it ScrapeStorm kept crashing, so this currently runs on my powerful desktop computer that is always on at home. When these run, the data flows in almost immediately, meaning if it's 4:05PM and you pulled the latest hour of data (which would be equivalent to the `max(***pkDummy***)`, more to come on that later on)

- Here is the size of both the `Octoparse` database and the `ScrapeStorm` db as of the last time this document was refreshed (updated daily):
```{r showTableSummary, echo=F, warning=F}
data.table(tables_summary)
```


**Back in MetaBase, let's click on the option that says `Octoparse`:**

![I would recommend starting here because this was the first/original database and will have more historical data compared to ScrapeStorm, which I got up and running much later](images/OctoparseClick.png)

Now you should see the tables that are contained within the `Octoparse` schema. By hovering over each table, you will see three options appear, which will be better explained in the [next section about *Documentation Usage*](#documentation-usage). In the screenshot the mouse is hovering over the `i` symbol for the Bitgur table:

![](images/OctoparseTableOptionsHover.png)

For now, let's go ahead and click on the name of the table `Bitgur`:

![](images/BitgurTableSelect.png)

After clicking on the table name, you should see some example data show up. This shows the first 2,000 rows of data found in the table:

![](images/BitgurPreview.png)

In the next section we will walk through some of the functionality associated with the things circled in red in the screenshot above using the Bitgur table as an example.


## Some other useful tables

- **`BitgurPerformance`**

    + This table is very similar to the `Bitgur` table we reviewed at the end of the last section, but this one includes some fields that summarize the short-term performance of the cryptocurrency in the short-term relative to when the data was collected:
    
    ![](images/BitgurPerformance.png)
    
    + Data collected since:
    
    + Row count as of 12/19/2019:
    
    
- **`ScrapeStorm.ShrimpyPrices`**

    + When using data from a table like the `BitgurPerformance` table just described, that data is aggregared at a global level. Meaning, the Bitgur website connects to a certain number of exchanges, and calculates things like the overall Market Capitalization ($), the 24h volume, etc. When it comes to the price, the website will average things across exchanges to get to the current price, which makes sense, but in my experience this is not as precise as it needs to be in order to programmatically trade on the cryptocurrency markets.
    
    + Therefore, a better approach is to collect the prices directly through the individual exchanges. Because I setup my trading system to work through a service called **Shrimpy**, I have been collecting prices for each exchange as reported through the Shrimpy website. This for example would be the page for the KuCoin exchange prices:
    
    ![](images/ShrimpyKucoin.png)
    

- **`ScrapesStorm.ShrimpyPricesBTC`**


