--- 
title: "Predict Crypto Database Quick Start Guide"
author: "Ricky Esclapon - riccardo.esclapon@colorado.edu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    config:
      sharing: null
      search: no
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is a quick start guide for the Predict Crypto database which should provide the support you need to interact with the database and pull data as you would like."
---

# Overview

[![](images/PredictCryptoLogo.PNG)](https://predictcryptodb.com)

This is a quick start guide for the [Predict Crypto DataBase](https://predictcryptodb.com) which should provide the support you need to interact with the database and pull data. Everything you need to know will be outlined in this document and you can use the sidebar on the left (`s` is the hotkey to show/hide it) to review the following sections:

1. [Overview-](#overview) This section.

2. [Interacting with the DB-](#interacting-with-the-db) Instructions around accessing the ***Metabase*** environment that will allow you to interact with the database, which is hosted on the website [PredictCryptoDB.com](https://predictcryptodb.com)

3. [Useful Tables-](#useful-tables) A review of some of the more useful/interesting tables you can find within the database and an overview of the best places to get started.

4. [Usage Guide-](#usage-guide) This guide explores some of the functionality found within the website/Metabase environment.

5. [Documentation Usage-](#documentation-usage) An overview of the documentation that is available through the Metabase environment and how to use it to answer questions you may have around where the data is sourced from as well as a complete data dictionary for every field in every table.

6. [Additional Tips-](#additional-tips) Some additional notes around using the environment to its full potential. This section goes over things like [pulling the most recent data](#pulling-most-recent-data), [creating dashboards](#creating-dashboards), [embedding anything you create as an iframe within a website or blog post](#embedding-anything-created-on-metabase), [creating e-mail triggers](#creating-e-mail-triggers) and [using a Python package to execute trades](#using-the-shrimpy-python-library).

*You can toggle the sidebar on the left side of the screen by pressing the letter `s` on your keyboard.*

<!--chapter:end:index.Rmd-->

# Interacting with the DB

First off, why the heck is there a website? If there's a database why wouldn't you just access it through a SQL editor like SSMS or MySQL? Let's start with the benefits of having this environment readily available to access through a website before jumping into how to start interacting with the database.

## Benefits
- Giving access to the database can be challenging, and having a centralized server that can be accessed through a website allows us to analyze the data without worrying about individual IP addresses being allowed access and other issues we would run into relating to authentication without this type of environment to work out of.

- There is no setup required on your part to start working with the database and as soon as your account is setup you can start writing SQL queries through the website and log into the website at any time. 

- You can access the Metabase interface to interact with the database from any device, like a tablet or smartphone.

- It works as a shared environment where we can all collaborate and see each other's dashboards, sql queries, visualizations etc... This is similar to the way a Tableau Online website would work within an organization.

- Makes creating dashboards to visualize the data extremely simple. Those dashboards can then be shared and be embedded, and the data can be refreshed on a schedule in a matter of clicks.

## "Metabase" explained 

First, let me clarify on what I mean by *Metabase* and how this all works. 

1. [Metabase](https://www.metabase.com/) is the name of the open source software that we are using to interact with the database.

2. Using the [docker container available to download on Metabase's website](https://hub.docker.com/r/metabase/metabase/), I stood up a server-type environment in AWS that hosts the Metabase instance that is connected to the database.

3. The AWS environment running the Metabase instance can be connected to by opening your web browser and navigating to the https secured website [predictcryptodb.com](https://predictcryptodb.com). From here, you will use the set of credentials that you created after clicking on the invitation from the e-mail you received. If you need an invite to create a set of credentials, e-mail me at riccardo.esclapon@colorado.edu.


## Logging In

1. To get started, the first thing you will want to do every time is go to the URL [predictcryptodb.com](https://predictcryptodb.com). You should see a login page:

    ![](images/MetabaseLogin.jpg)
    
    + If you are having issues accessing the website, try spelling the url including [***https://***predictcryptodb.com](https://predictcryptodb.com). 

    + Before you can login, you need to configure your account through the e-mail you should have received. If you did not receive the e-mail, check your spam folder first:

        ![*If you need the e-mail to setup your login, e-mail me*](images/MetabaseInvite.jpg)
    
        + Configure your password and you will be able to use your e-mail and password to login in the future:
    
        ![](images/MetabaseSetPassword.png)

2. Once you have logged in, you should see a page that looks like this:

    ![](images/MetabaseDashboard.png)

In the next section we will talk about some good tables to get started on, and getting more comfortable navigating the layout of the website.






<!--chapter:end:02-InteractWithDB.Rmd-->

# Useful Tables

```{r dbConnect, echo=F, warning=F}
library(DBI)
library(RMySQL)
library(data.table)
library(knitr)
library(rmarkdown)
library(DT)
library(reticulate)

############# SQL CONNECTION ###############
getSqlConnection <- function(){
  con <-
    dbConnect(
      RMySQL::MySQL(),
      username = Sys.getenv('user'),
      password = Sys.getenv('pswd'),
      host = Sys.getenv('ipAddress'),
      dbname = 'Octoparse'
    )
  return(con)
}

database_connection <- getSqlConnection()
tables_list <- dbListTables(database_connection)

query <- 'SELECT table_schema "DB name", sum( data_length + index_length ) / 1024 / 1024 "Size in MB", curdate() as Today 
FROM information_schema.TABLES GROUP BY table_schema 
ORDER BY `Size in MB` desc limit 2'

tables_summary <- dbFetch(dbSendQuery(database_connection, query))
```

In this section we will be reviewing some interesting tables and good places to get started.

## Browse Data

Let's start by clicking on the `Browse Data` tab in the top right of the Metabase environment:

![](images/MetabaseBrowseData.png)


Here, you will see two different options:

![](images/DBSelection.png)

- `Octoparse` is the schema that is associated with data I have collected by using the *Octoparse* web scraping software.

- Conversely, `ScrapeStorm` is the schema associated with data collected using the *ScrapeStorm* web scraping software.

- You should also see an option for `PredictCryptoPredictions`. This schema does not have much in it today, but over time as I do more predictive modeling it will populate with new tables for new predictive models and is used to simulate model performance before starting to programmatically trade using the predictions. This guide ignores this schema/database for now to focus on the raw data itself, which always comes from `Octoparse` and `ScrapeStorm`.

- Web scraping has its challenges in terms of stability, so I built some additional resilience by using two different tools that work independently of each other and do similar things (and in some cases collect the same data). Although not a perfect solution, having both up and running means we can usually fill the gaps that might arise in each tool respectively.

    <!-- + The web scrapers that are run through Octoparse run on their servers in the cloud, which are very stable but have had some issues here and there in the past. -->

    <!-- + The web scrapers that are run through ScrapeStorm run on a local machine on my end. I have a computer in the cloud that runs 24/7 but even after upgrading the hardware on it ScrapeStorm kept crashing, so this currently runs on my powerful desktop computer that is always on at home. When these run, the data flows in almost immediately, meaning if it's 4:05PM and you pulled the latest hour of data (which would be equivalent to the ***`max(pkDummy)`***, more to come on that later on) -->

**Back in MetaBase, let's click on the option that says `Octoparse`:**

![I would recommend starting here because this was the first/original database and will have more historical data compared to ScrapeStorm, which I got up and running much later](images/OctoparseClick.png)

- Now you should see the tables that are contained within the `Octoparse` schema. By hovering over each table, you will see three options appear, which will be better explained in the [next section about *Documentation Usage*](#documentation-usage). In the screenshot the mouse is hovering over the `i` symbol for the Bitgur table:

    ![](images/OctoparseTableOptionsHover.png)

    - By clicking on the middle button that says *Learn more about this table*, you will be brought to its documentation:
    
        ![](images/LearnMoreAboutThisTable.png)

For now, let's go ahead and click on the name of the table `Bitgur`:

![](images/BitgurTableSelect.png)

After clicking on the table name, you should see some example data show up. This shows the first 2,000 rows of data found in the table:

![](images/BitgurPreview.png)

In the next section [Usage Guide](#usage-guide) we will walk through some of the functionality associated with the things circled in red in the screenshot above using the Bitgur table as an example.


## Useful tables
- For the previews below, keep an eye out for a button to show more columns:
![](images/MoreColumns.png)

- Things will tend to live as chr/strings within the database because I found that saving everything as a string prevents schema conflicts from no longer uploading data to the database after it gets collected. The previews below will show you the data types as well, so just keep in mind you might have to change some data types after extracting the data from the database.

### Tables in `Octoparse` db


#### [**`Bitgur`**](https://predictcryptodb.com/question/6)
Bitgur is a website that aggregates cryptocurrency prices and offers tools to analyze the cryptocurrency markets by connecting directly to the API of the different exchanges in a very similar way to the way the most famous website in this space [CoinMarketCap](coinmarketcap.com) does. It's important to note however, that CoinMarketCap connects to 306 exchanges and Bitgur connects to only 56 exchanges (as of 01/01/2020); this results in dramatically different values in terms of volume, but the price and market capitalization would track very closely between the two websites. I don't currently scrape data from CoinMarketCap because they prohibit this type of behavior in their website usage terms in order to sell an extremely expensive API.

```{r BitgurPreview, echo=F}
options(scipen=999) #get rid of scientific notation 

query <- "SELECT * FROM Bitgur limit 5"
# this better but takes longer: 
#query <- "select * from Bitgur where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
- Source: [https://bitgur.com/](https://bitgur.com/)

- Data collected since: 2018-11-11

<!-- - Number of rows:  -->
<!-- ```{r BitgurOctoparseRowsSummary, echo=F} -->
<!-- query <- "SELECT TABLE_ROWS as 'Rows', NOW() as 'Checked on' FROM INFORMATION_SCHEMA.TABLES Where (TABLE_SCHEMA = 'Octoparse'and TABLE_NAME = 'Bitgur')" -->

<!-- dbFetch(dbSendQuery(database_connection, query)) -->
<!-- ``` -->

#### [**`BitgurPerformance`**](https://predictcryptodb.com/question/7)
```{r BitgurPerformancePreview, echo=F}
query <- "SELECT * FROM BitgurPerformance limit 5"
# this better but takes longer: 
#query <- "select * from BitgurPerformance where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
    
- This table is very similar to the `Bitgur` table, but this one includes some fields that summarize the short-term performance of the cryptocurrency in the short-term relative to when the data was collected. 
       
- The normal `Bitgur` table would have the prices fields, but not information about the performance over the previous 1h, 3h, etc..
    
- Source: [https://bitgur.com/performance](https://bitgur.com/performance)
        
- Data collected since: 2019-11-05



#### [**`CoinCheckup`**]()



#### [**`Prices`**]()



#### [**`CoinToBuy`**]()



#### [**`TechnicalAnalysis`**](https://predictcryptodb.com/question/10)
This web scraper collects data from the website [Investing.com](https://www.investing.com/crypto/). This website offers free Technical Analysis indicators on traditional markets, and they adapted that for the cryptocurrency markets. In order to get a full summary of the Technical Analysis outlooks, the web scraper needs to iterate through each cryptocurrency individually, and this is one of the few web scrapers that takes too long to run every hour. 

```{r TechnicalAnalysisPreview, echo=F}
options(scipen=999) #get rid of scientific notation 

query <- "SELECT * FROM TechnicalAnalysis limit 5"
# this better but takes longer: 
#query <- "select * from TechnicalAnalysis where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
- Source: [https://www.investing.com/crypto/](https://www.investing.com/crypto/)

- Data collected since: 2018-11-15


### Tables in `ScrapeStorm` db
    
#### [**`ScrapeStorm.ShrimpyPrices`**](https://predictcryptodb.com/question/8)

- When using data from a table like the `BitgurPerformance`, the data is aggregared at a global level. Meaning, the Bitgur website connects to a certain number of exchanges, and calculates things like the overall Market Capitalization ($), the 24h volume, etc. When it comes to the price, the website will average things across exchanges to get to the current price, which makes sense, but in my experience this is not as precise as it needs to be in order to programmatically trade on the cryptocurrency markets.
    
    + Therefore, a better approach is to collect the prices directly through the individual exchanges. Because I setup my trading system to work through a service called **Shrimpy**, I have been collecting prices for each exchange as reported through the Shrimpy website. This for example would be the page for the KuCoin exchange prices:

```{r ShrimpyPricesPreview, echo=F}
query <- "SELECT * FROM ScrapeStorm.ShrimpyPrices limit 5"
# this better but takes longer: 
#query <- "select * from ScrapeStorm.ShrimpyPrices where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
    
- Source: [https://www.shrimpy.io/exchange](https://www.shrimpy.io/exchange)

    ![Example for the KuCoin exchange](images/ShrimpyKucoin.png)

- Data collected since: 


#### [**`ScrapesStorm.ShrimpyPricesBTC`**](https://predictcryptodb.com/question/9)

This table is exactly like the previous one `ShrimpyPrices`, but this one shows prices in BTC instead of being in USD.

```{r ShrimpyPricesBTCPreview, echo=F}
#query <- "SELECT * FROM ScrapeStorm.ShrimpyPricesBTC limit 5"
# this better but takes longer: 
query <- "select * from ScrapeStorm.ShrimpyPricesBTC order by pkDummy desc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```

- Source: [https://www.shrimpy.io/exchange](https://www.shrimpy.io/exchange)

- Data collected since: 



<!-- ### Tables for predictive models -->
<!-- - For the PredictCrypto project, I have been working on different iterations of the predictive models to predict and trade on the live cryptocurrency markets. For an overview of what this process looks like from start to finish, please see the **Alteryx Use Case** for the project: [https://community.alteryx.com/t5/Alteryx-Use-Cases/Predicting-and-Trading-on-the-Cryptocurrency-Markets-using/ta-p/494058](https://community.alteryx.com/t5/Alteryx-Use-Cases/Predicting-and-Trading-on-the-Cryptocurrency-Markets-using/ta-p/494058) -->

<!-- - As I improve things on the predictive modeling side of things, I am going to create different iterations of the model and write out predictions made in real time by the newest models and save those predictions so I can analyze what would have happened by actually trading on them. -->

<!-- - Currently   -->


### Database size info

**Size in MB of both the `Octoparse` database and the `ScrapeStorm` db as of the last time this document was refreshed (updated daily):**
```{r showTableSummary, echo=F, warning=F}
kable(format(tables_summary, big.mark = ",",nsmall = 2))
```

**Number of rows by table:**
```{r showTablesSummary, echo=F, results='asis'}
query <- "SELECT TABLE_SCHEMA as 'Database', TABLE_NAME as 'Table Name', TABLE_ROWS as 'Rows' FROM INFORMATION_SCHEMA.TABLES Where (TABLE_SCHEMA = 'Octoparse' or TABLE_SCHEMA = 'ScrapeStorm') and TABLE_ROWS > 1 and UPDATE_TIME > DATE_SUB(NOW(), INTERVAL 5 DAY) Order by Rows desc, UPDATE_TIME desc"

kable(format(dbFetch(dbSendQuery(database_connection, query)), big.mark = ","))
```




<!--chapter:end:03-UsefulTables.Rmd-->

# Usage Guide




## Pulling Most Recent Data

Let's say you went ahead and trained a predictive model using the data found in the database and now you want to test making some *semi-live* predictions, how would you go about pulling the latest data to make predictions on?

- pkDummy

- pkey

Make a note that can't do this between different schemas because the Octoparse one is in UTC and the ScrapeStorm one in local time. Only real effective way I found do to that is through Alteryx tools.


## Creating Tables (LOTS OF JUNK!)

Let's explore some of the functionality found in Metabase using the `Bitgur` table results page we got to at the end of the previous section.

- The first thing I want to point out, is that you can click on each field to open a menu with an assortment of options based on the data type. For example, if we click on the **Price** field, these are the options we are given:

    ![](images/PriceFieldOptions.png)

    + The Date field is a different data types and offers different options. For example we could sort the data by the **Date** field **Descending**:
    
        ![](images/DateDescendingOption.png)
    
        + Which would return the first 2,000 rows again but this time sorted showing the largest values first (meaning the latest date):
        
            ![If the `Bitgur` table is not being very responsive, try using the `BitgurPerformance` table instead, which should load faster (but has less historical data, this table is described in [a section below](#some-other-useful-tables))](images/DateDescendingOutcome.png)

- Next, let's click on the purple `Filter` button in the top right of the page:

    ![](images/FilterButton.png)
    
    - Now you should see the following sidebar pop-up:
    
        ![Notice how the icons next to the field names correspond to their data types](images/FilterSidebar.png)
    
        + Let's do a quick example filtering based on the **DateTime** field:
        
            ![](images/DateTimeClick.png)
        
            + Let's add a filter to only select data where the **DateTime** field is from the last 7 days:
            
            ![*please be patient as this loads or do the same with a different table that contains less historical data*](images/DateTimeOptions.png)
            
         
- Now that we have filtered the data down to the subset we are interested in, let's adjust the look of the table using the `Settings` option in the bottom left:

    ![](images/BitgurTableSettings.png)
    
    + After clicking on the `Settings` button, you should see a sidebar show up on the left. First, let's edit the settings for the **Price** field:
        
        ![](images/BitgurPriceSettings.png)
            
        + And let's set the currency to be displayed with every cell instead of in parenthesis in the column name:
            
            ![](images/PriceShowCurrency.png)
                
            + Now the currency field will look like this for each row:
            
            ![](images/BitgurPriceModified.png)
                
    + Let's also do a quick edit to the **Volatility** field settings:
        
        ![](images/BitgurVolatilitySettings.png)
        
        + Enable the setting to `Show a mini bar chart`:
            
        ![](images/ShowMiniBarChart.png)
                
        + Now each row should have a bar that is filled based on the current value compared to the minimum and maximum values for that field for the current subset of the data:
                
            ![](images/VolatilityMiniBarChart.png)
                
            + If we sort the data by **Volatility** in *descending* order, we can see the row that sets the bar for the maximum for the rest of the rows:
                    
            ![](images/MaximumVolatility.png)
                        
         + Now let's also add some conditional formatting to the **Volatility** field, which will color each row when a certain criteria is met. Go back to the `Settings` page of the table, and this time click on `Conditional Formatting`:
         
             ![](images/ConditionalFormattingSelect.png)
             
             + Now we can create a rule to highlight rows with a value for **Volatility > 50**:
             
             ![](images/ConditionalFormattingRule.png)
             
             + The table's rows should now be highlighted when the criteria is met:
             
             ![](images/ConditionalFormattingOutput.png)


### Creating Visualizations


### Exporting Data

(make a note about confidentiality of the data)



## Answer Questions


## Creating dashboards


<!--chapter:end:04-UsageGuide.Rmd-->

# Documentation Usage

Everything you have visibility to within the Metabase environment is documented. Meaning, every table you see has a description which includes the URL of where the data is collected (as well as the web scraping methodology where informative), and every field within those tables has also been documented on [predictcryptodb.com](https://predictcryptodb.com). Every table you have visibility to also has an e-mail alert system that alerts me if the data stops flowing through on an hourly (or daily for some) basis. If anything becomes broken I will remove the visibility of that table within Metabase, so if a table is found within the Metabase environment you can assume the web scraper for that table is still functioning correctly and that the data is uploaded to the table on an automated schedule. I think Metabase could do a better job at making the documentation for the different tables more intutitive to find, so hopefully this guide can provide some clarity where Metabase might be lacking.

## pkDummy and pkey


## Navigating the Metabase interface



## Using x-rays to explore a table

- caveat that the database doesn't have the greatest resources that a big company would be able to provide and how this database costs around $220 a month to maintain vs. what a company would spend


(could really use a section on explaining the way pkDummy, pkey work and how they follow two different time zones, show Alteryx trick to deal with the trickiness of UTC not having daylight savings and our local times having it)

<!--chapter:end:05-DocumentationUsage.Rmd-->

# Additional Tips


## Creating Variables for queries

- Within new question, click on the "x" symbol to open sidebar on the side and learn how to use variables:

    + {{variable_name}} creates a variable in this SQL template called "variable_name". Variables can be given types in the side panel, which changes their behavior. All variable types other than "Field Filter" will automatically cause a filter widget to be placed on this question; with Field Filters, this is optional. When this filter widget is filled in, that value replaces the variable in the SQL template.


## Embedding anything created on Metabase


## Creating e-mail triggers
The Metabase environment is setup to allow for e-mail alerts, where you can e-mail out results on a schedule.

- All tables are configured to have e-mail triggers that check the data is still flowing in as expected. If that is not the case, there are queries for each which would produce a result and trigger an alert. 

- At the start of every hour, there is an alert that checks for queries that have been running longer than 20 minutes and e-mails me the SQL statements to kill those queries.

- When creating a table or chart in Metabase, you will have the option in the bottom right of the page to create an e-mail alert by clicking on the button shaped like a bell:

    ![](images/AlertsBell.PNG)


### Pulses


## Using the Shrimpy-Python Library

This section is a bit more *expert level* and I don't really expect anyone to go down this route, but I figured it was worth having a small section on using the Shrimpy software for trading cryptocurrencies by using their Python package. This is a particularly good way to execute trades on the cryptocurrency markets because rather than having to connect to and manage each API individually, you can connect up to 16 exchanges to your Shrimpy account and programmatically trade on every one of them using the same set of functions in Python.

- First you would have to import the `shrimpy-python` library. For installation instructions see this [link]()


<!-- ```{python eval=FALSE} -->
<!-- import shrimpy-python -->
<!-- ``` -->




```{r disconnectDB, echo=F}
dbDisconnect(database_connection)
```


<!--chapter:end:06-AdditionalTips.Rmd-->

