# Useful Tables

```{r dbConnect, echo=F, warning=F}
library(DBI)
library(RMySQL)
library(data.table)
library(knitr)
library(rmarkdown)
library(DT)

############# SQL CONNECTION ###############
source('login.R')
getSqlConnection <- function(){
  con <-
    dbConnect(
      RMySQL::MySQL(),
      username = db_user,
      password = db_pswd,
      host = db_ip,
      dbname = 'Octoparse'
    )
  return(con)
}

database_connection <- getSqlConnection()
tables_list <- dbListTables(database_connection)

query <- 'SELECT table_schema "DB name", sum( data_length + index_length ) / 1024 / 1024 "Size in MB", curdate() as Today 
FROM information_schema.TABLES GROUP BY table_schema 
ORDER BY `Size in MB` desc limit 3' #if new schemas increase the limit!

tables_summary <- dbFetch(dbSendQuery(database_connection, query))
```

In this section we will be reviewing some interesting tables and good places to get started.

## Browse Data

Let's start by clicking on the `Browse Data` tab in the top right of the Metabase environment:

![](images/MetabaseBrowseData.png)


Here, you should see the options `Octoparse` and `ScrapeStorm`:

![](images/DBSelection.png)

- `Octoparse` is the schema that is associated with data I have collected by using the *Octoparse* web scraping software.

- Conversely, `ScrapeStorm` is the schema associated with data collected using the *ScrapeStorm* web scraping software.

- You should also see an option for `PredictCryptoPredictions`. This schema does not have much in it right now, but over time as I do more predictive modeling it will populate with new tables for new predictive models and is used to simulate model performance before starting to programmatically trade using the predictions. This guide ignores this schema/database for now to focus on the raw data itself, which always comes from the databases `Octoparse` and `ScrapeStorm`.

    <!-- + The web scrapers that are run through Octoparse run on their servers in the cloud, which are very stable but have had some issues here and there in the past. -->

    <!-- + The web scrapers that are run through ScrapeStorm run on a local machine on my end. I have a computer in the cloud that runs 24/7 but even after upgrading the hardware on it ScrapeStorm kept crashing, so this currently runs on my powerful desktop computer that is always on at home. When these run, the data flows in almost immediately, meaning if it's 4:05PM and you pulled the latest hour of data (which would be equivalent to the ***`max(pkDummy)`***, more to come on that later on) -->

**Back in MetaBase, let's click on the option that says `Octoparse`:**

![I would recommend starting here because this was the first/original database and will have more historical data compared to ScrapeStorm, which I got up and running much later](images/OctoparseClick.png)

- Now you should see the tables that are contained within the `Octoparse` schema. By hovering over each table, you will see three options appear, which will be better explained in the [next section about *Documentation Usage*](#documentation-usage). In the screenshot the mouse is hovering over the `i` symbol for the Bitgur table:

    ![](images/OctoparseTableOptionsHover.png)

    - By clicking on the middle button that says *Learn more about this table*, you will be brought to its documentation:
    
        ![](images/LearnMoreAboutThisTable.png)

For now, let's go ahead and click on the name of the table `Bitgur`:

![](images/BitgurTableSelect.png)

After clicking on the table name, you should see some example data show up. This shows the first 2,000 rows of data found in the table:

![](images/BitgurPreview.png)

In the next section [Usage Guide](#usage-guide) we will walk through some of the functionality associated with the things circled in red in the screenshot above using the Bitgur table as an example.


## Useful tables
- For the previews below, keep an eye out for a button to show more columns:
![](images/MoreColumns.png)

- Things will tend to live as chr/strings within the database because I found that saving everything as a string prevents schema conflicts from no longer uploading data to the database after it gets collected. The previews below will show you the data types as well, so just keep in mind you might have to change the data types sometimes after extracting the data from the database.

- The data shown below should be no more than 1 day old. The latest data is shown for each table and this document refreshes automatically daily.

### Tables in `Octoparse` db
*All date/time fields in the Octoparse database are in [UTC](https://www.google.com/search?q=time+utc)*

#### [**`Bitgur`**](https://predictcryptodb.com/question/6)
<!-- Bitgur is a website that aggregates cryptocurrency prices and offers tools to analyze the cryptocurrency markets by connecting directly to the API of the different exchanges in a very similar way to the way the most famous website in this space [CoinMarketCap](coinmarketcap.com) does. It's important to note however, that CoinMarketCap connects to 306 exchanges and Bitgur connects to only 56 exchanges (as of 01/01/2020); this results in dramatically different values in terms of volume, but the price and market capitalization would track very closely between the two websites. I don't currently scrape data from CoinMarketCap because they prohibit this type of behavior in their website usage terms in order to sell an extremely expensive API. -->

```{r BitgurPreview, echo=F}
options(scipen=999) #get rid of scientific notation 

#REMEMBER that if I want to make things faster and have book build take 2 seconds un-comment the first set of queries and comment out the second longer query for all tables

#query <- "SELECT * FROM Bitgur limit 5"
# this better but takes longer: 
query <- "select * from Bitgur where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.bitgur.com](https://bitgur.com/) -->

- Data collected since: 2018-11-11

- Documentation: [predictcryptodb.com/reference/databases/2/tables/36](https://predictcryptodb.com/reference/databases/2/tables/36)


#### [**`BitgurPerformance`**](https://predictcryptodb.com/question/7)
```{r BitgurPerformancePreview, echo=F}
#query <- "SELECT * FROM BitgurPerformance limit 5"
# this better but takes longer: 
query <- "select * from BitgurPerformance where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.bitgur.com/performance](https://bitgur.com/performance) -->
        
- Data collected since: 2019-11-05

- Documentation: [predictcryptodb.com/reference/databases/2/tables/60](https://predictcryptodb.com/reference/databases/2/tables/60)


#### [**`CoinCheckup`**](https://predictcryptodb.com/question/11)
```{r CoinCheckupPreview, echo=F}
#query <- "SELECT * FROM CoinCheckup limit 5"
# this better but takes longer: 
query <- "select * from CoinCheckup where Rank > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.coincheckup.com](https://coincheckup.com) -->

- Data collected since: 2018-12-04

- Documentation: [predictcryptodb.com/reference/databases/2/tables/587](https://predictcryptodb.com/reference/databases/2/tables/587)


#### [**`CoinCheckupDetails`**](https://predictcryptodb.com/question/12)
```{r CoinCheckupDetailsPreview, echo=F}
#query <- "SELECT * FROM CoinCheckupDetails limit 5"
# this better but takes longer: 
query <- "select * from CoinCheckupDetails Where Name > '' order by pkDummy desc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.coincheckup.com](https://coincheckup.com/coins/bitcoin/analysis) -->

- Data collected since: 2019-10-01

- Documentation: [predictcryptodb.com/reference/databases/2/tables/591](https://predictcryptodb.com/reference/databases/2/tables/591)



#### [**`CoinStatsPrices`**](https://predictcryptodb.com/question/13)
```{r CoinStatsPricesPreview, echo=F, warning=F}
#query <- "SELECT * FROM CoinStatsPrices limit 5"
# this better but takes longer: 
query <- "select * from CoinStatsPrices where Rank > 0 and Price > 0 order by pkDummy desc, rank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.coinstats.app](https://coinstats.app/) -->

- Data collected since: 2018-11-11

- Documentation: [predictcryptodb.com/reference/databases/2/tables/28](https://predictcryptodb.com/reference/databases/2/tables/28)


#### [**`CoinToBuy`**](https://predictcryptodb.com/question/14)
```{r CoinToBuyPreview, echo=F}
#query <- "SELECT * FROM CoinToBuy limit 5"
# this better but takes longer: 
query <- "select * from CoinToBuy where SafetyRank > 0 order by pkDummy desc, SafetyRank asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.cointobuy.io](www.cointobuy.io) -->

- Data collected since: 2018-11-16

- Documentation: [predictcryptodb.com/reference/databases/2/tables/26](https://predictcryptodb.com/reference/databases/2/tables/26)


#### [**`TechnicalAnalysis`**](https://predictcryptodb.com/question/10)
```{r TechnicalAnalysisPreview, echo=F}
options(scipen=999) #get rid of scientific notation 

#query <- "SELECT * FROM TechnicalAnalysis limit 5"
# this better but takes longer: 
query <- "select * from TechnicalAnalysis order by pkDummy desc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
<!-- - Source: [www.investing.com/crypto/](https://www.investing.com/crypto/) -->

- Data collected since: 2018-11-15

- Documentation: [predictcryptodb.com/reference/databases/2/tables/61](https://predictcryptodb.com/reference/databases/2/tables/61)


### Tables in `ScrapeStorm` db
*Any date/time fields in the ScrapeStorm database are in the MST timezone (Colorado time)*

#### [**`Messari`**](https://predictcryptodb.com/question/15)
```{r MessariPreview, echo=F}
#query <- "SELECT * FROM ScrapeStorm.Messari limit 5"
# this better but takes longer: 
query <- "select * from ScrapeStorm.Messari where cast(Rank as unsigned) > 0 order by pkDummy desc, cast(Rank as unsigned) asc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
    
<!-- - Source: [messari.io/screener/my-screener-5385A1A5](https://messari.io/screener/my-screener-5385A1A5) -->

- Data collected since: 2019-09-26

- Documentation: [predictcryptodb.com/reference/databases/4/tables/479](https://predictcryptodb.com/reference/databases/4/tables/479)
    
#### [**`ShrimpyPrices`**](https://predictcryptodb.com/question/8)
```{r ShrimpyPricesPreview, echo=F}
#query <- "SELECT * FROM ScrapeStorm.ShrimpyPrices limit 5"
# this better but takes longer: 
query <- "select * from ScrapeStorm.ShrimpyPrices order by pkDummy desc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```
    
<!-- - Source: [www.shrimpy.io/exchange](https://www.shrimpy.io/exchange) -->

    <!-- ![Example for the KuCoin exchange](images/ShrimpyKucoin.png) -->

- Data collected since: 2019-11-16

- Documentation: [predictcryptodb.com/reference/databases/4/tables/467](https://predictcryptodb.com/reference/databases/4/tables/467)

#### [**`ShrimpyPricesBTC`**](https://predictcryptodb.com/question/9)
```{r ShrimpyPricesBTCPreview, echo=F}
#query <- "SELECT * FROM ScrapeStorm.ShrimpyPricesBTC limit 5"
# this better but takes longer: 
query <- "select * from ScrapeStorm.ShrimpyPricesBTC order by pkDummy desc limit 5"

paged_table(dbFetch(dbSendQuery(database_connection, query)))
```

<!-- - Source: [www.shrimpy.io/exchange](https://www.shrimpy.io/exchange) -->

- Data collected since: 2019-11-16

- Documentation: [predictcryptodb.com/reference/databases/4/tables/454](https://predictcryptodb.com/reference/databases/4/tables/454)


<!-- #### [**`BitgurBackup`**](https://predictcryptodb.com/question/17) -->
<!-- ```{r BitgurBackupBTCPreview, echo=F} -->
<!-- query <- "SELECT * FROM ScrapeStorm.BitgurBackup limit 5" -->
<!-- # this better but takes longer:  -->
<!-- #query <- "select * from ScrapeStorm.BitgurBackup order by pkDummy desc limit 5" -->

<!-- paged_table(dbFetch(dbSendQuery(database_connection, query))) -->
<!-- ``` -->

<!-- - Source: [www.bitgur.com](https://bitgur.com/) -->

<!-- - Data collected since: 2019-11-11 -->

<!-- - Documentation: [predictcryptodb.com/reference/databases/4/tables/594](https://predictcryptodb.com/reference/databases/4/tables/594) -->


### PredictCryptoPredictions db
- For the PredictCrypto project, I have been working on different iterations of the predictive models to predict and trade on the live cryptocurrency markets. For an overview of what this process looks like from start to finish, please see the **Alteryx Use Case** for the project: [https://community.alteryx.com/t5/Alteryx-Use-Cases/Predicting-and-Trading-on-the-Cryptocurrency-Markets-using/ta-p/494058](https://community.alteryx.com/t5/Alteryx-Use-Cases/Predicting-and-Trading-on-the-Cryptocurrency-Markets-using/ta-p/494058)

- As I improve things on the predictive modeling side of things, I am going to create different iterations of the model and write out predictions made in real time by the newest models and save those predictions so I can analyze what would have happened by actually trading on them. Once I have done more progress I will provide more tools and better documentation around analyzing the performance of the different predictive models.


### Database size info

**Size in MB of both the `Octoparse` database and the `ScrapeStorm` db as of the last time this document was refreshed (updated daily):**
```{r showTableSummary, echo=F, warning=F}
kable(format(tables_summary, big.mark = ",",nsmall = 2))
```

**Number of rows by table:**
```{r showTablesSummary, echo=F, results='asis'}
query <- "SELECT TABLE_SCHEMA as 'Database', TABLE_NAME as 'Table Name', TABLE_ROWS as 'Rows' FROM INFORMATION_SCHEMA.TABLES Where (TABLE_SCHEMA = 'Octoparse' or TABLE_SCHEMA = 'ScrapeStorm') and TABLE_ROWS > 1 and UPDATE_TIME > DATE_SUB(NOW(), INTERVAL 5 DAY) and (TABLE_NAME not like 'v2%') and (TABLE_NAME not like 'AYX%')  order by Rows desc, UPDATE_TIME desc"

kable(format(dbFetch(dbSendQuery(database_connection, query)), big.mark = ","))
```

### Why two web scraping tools?
Web scraping has its challenges in terms of stability, so I built some additional resilience by using two different tools that work independently of each other and do similar things (and in some cases collect the same data). Although not a perfect solution, having both up and running means we can usually fill the gaps that might arise in each tool respectively.

